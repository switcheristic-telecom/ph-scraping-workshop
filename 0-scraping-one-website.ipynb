{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# replace the url with the one you want!\n",
    "\n",
    "\n",
    "# url = \"http://www.geocities.com/Heartland/Fields/5727/\"\n",
    "url = \"http://www.olink.com.cn/mylove/\"\n",
    "# url = \"http://www.pagina12web.com.ar/\"\n",
    "# url = \"http://www.elespectador.com/html/i_portals/index.php\"\n",
    "# url = \"http://www.dawn.com/2006/02/15/index.htm\"\n",
    "\n",
    "\n",
    "def get_md5(url):\n",
    "    return hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "\n",
    "url_md5 = get_md5(url)\n",
    "\n",
    "print(\"URL: \" + url)\n",
    "print(\"MD5(id): \" + url_md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def make_wm_cdx_url(url, from_time=\"19960101\", to_time=\"20051231\"):\n",
    "    \"\"\"\n",
    "    Construct a URL to query the Wayback Machine CDX API\n",
    "    for a given URL and time range\n",
    "    \"\"\"\n",
    "    base_url = \"https://web.archive.org/cdx/search/cdx\"\n",
    "    params = {\n",
    "        \"url\": url,\n",
    "        \"from\": from_time,\n",
    "        \"to\": to_time,\n",
    "    }\n",
    "\n",
    "    # this will create a URL with the parameters\n",
    "    # eg. https://web.archive.org/cdx/search/cdx?url=example.com&from=19960101&to=20051231\n",
    "    url_with_params = requests.Request(\"GET\", base_url, params=params).prepare().url\n",
    "    return url_with_params\n",
    "\n",
    "\n",
    "wayback_cdx_url = make_wm_cdx_url(url)\n",
    "print(\"Wayback CDX URL:\")\n",
    "print(wayback_cdx_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv(\n",
    "    wayback_cdx_url,\n",
    "    names=[\n",
    "        \"urlkey\",\n",
    "        \"timestamp\",\n",
    "        \"original\",\n",
    "        \"mimetype\",\n",
    "        \"statuscode\",\n",
    "        \"digest\",\n",
    "        \"length\",\n",
    "    ],\n",
    "    sep=\"\\s+\",\n",
    ")\n",
    "\n",
    "print(\"Dataframe:\")\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_list = dataframe.to_dict(\"records\")\n",
    "print(f\"Found {len(snapshot_list)} snapshots.\")\n",
    "print(\"Snapshot list:\")\n",
    "print(snapshot_list)\n",
    "\n",
    "\n",
    "useful_snapshots = []\n",
    "\n",
    "for snapshot in snapshot_list:\n",
    "    if snapshot[\"statuscode\"] != 200:\n",
    "        continue\n",
    "    snapshot_url = f\"https://web.archive.org/web/{snapshot['timestamp']}if_/{url}\"\n",
    "\n",
    "    useful_snapshot = {\n",
    "        \"timestamp\": snapshot[\"timestamp\"],\n",
    "        \"url\": snapshot_url,\n",
    "    }\n",
    "    useful_snapshots.append(useful_snapshot)\n",
    "\n",
    "print(f\"Kept {len(useful_snapshots)} snapshots.\")\n",
    "print(\"Useful snapshots:\")\n",
    "print(useful_snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "DATA_DIR = \"example-data\"\n",
    "URL_DIR = os.path.join(DATA_DIR, url_md5)\n",
    "\n",
    "\n",
    "def download_file(url, dest, retries=3, sleep=1):\n",
    "    if os.path.isfile(dest):\n",
    "        print(f\"File {dest} already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} to {dest}\")\n",
    "    for i in range(retries):\n",
    "        print(f\"Attempt {i+1}/{retries}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            with open(dest, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            time.sleep(sleep)\n",
    "\n",
    "\n",
    "sleep_time_per_snapshot = 1\n",
    "\n",
    "max_retries = 3\n",
    "\n",
    "for snapshot in useful_snapshots:\n",
    "    snapshot_url = snapshot[\"url\"]\n",
    "    print(snapshot_url)\n",
    "\n",
    "    snapshot_dir = os.path.join(URL_DIR, str(snapshot[\"timestamp\"]))\n",
    "    os.makedirs(snapshot_dir, exist_ok=True)\n",
    "    snapshot_file = os.path.join(snapshot_dir, \"snapshot.html\")\n",
    "    download_file(snapshot_url, snapshot_file)\n",
    "\n",
    "    time.sleep(sleep_time_per_snapshot)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import codecs\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "DATA_DIR = f\"example-data/{url_md5}\"\n",
    "WAYBACK_BASE_URL = \"https://web.archive.org\"\n",
    "\n",
    "html_files = []\n",
    "# walk through all files in the DATA_DIR\n",
    "# and find all HTML files\n",
    "html_extensions = [\".html\", \".htm\"]\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(tuple(html_extensions)):\n",
    "            file_path = os.path.join(root, file)\n",
    "            html_files.append(file_path)\n",
    "\n",
    "\n",
    "def download_file(url, dest, retries=3, sleep=1):\n",
    "    if os.path.isfile(dest):\n",
    "        print(f\"File {dest} already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} to {dest}\")\n",
    "    for i in range(retries):\n",
    "        print(f\"Attempt {i+1}/{retries}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            with open(dest, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            time.sleep(sleep)\n",
    "\n",
    "\n",
    "print(f\"Found {len(html_files)} HTML files.\")\n",
    "\n",
    "for file_path in html_files:\n",
    "    print(\"Processing file:\", file_path)\n",
    "    html_dir = os.path.dirname(file_path)\n",
    "\n",
    "    html_content = None\n",
    "    try:\n",
    "        with codecs.open(file_path, \"r\", \"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e} in utf-8, trying latin-1\")\n",
    "\n",
    "    try:\n",
    "        with codecs.open(file_path, \"r\", \"latin-1\") as f:\n",
    "            html_content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e} in latin-1\")\n",
    "\n",
    "    if not html_content:\n",
    "        print(\"Error reading file, skipping...\")\n",
    "        continue\n",
    "\n",
    "    soup = bs4.BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    ########## Extract basic metadata ##########\n",
    "    title = soup.title.string\n",
    "    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    if description:\n",
    "        description = description[\"content\"]\n",
    "    else:\n",
    "        description = None\n",
    "\n",
    "    metadata = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(html_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    ########## Extract links ##########\n",
    "    links = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href:\n",
    "            links.append(href)\n",
    "\n",
    "    links_path = os.path.join(html_dir, \"links.json\")\n",
    "    with open(links_path, \"w\") as f:\n",
    "        json.dump(links, f, indent=2)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract sources #############\n",
    "    ########################################\n",
    "\n",
    "    sources = []\n",
    "    tags = [\n",
    "        \"link\",\n",
    "        \"script\",\n",
    "        \"iframe\",\n",
    "        \"embed\",\n",
    "        \"audio\",\n",
    "        \"video\",\n",
    "        \"source\",\n",
    "        \"track\",\n",
    "        \"object\",\n",
    "        \"img\",\n",
    "    ]\n",
    "    # getting all the `src` attributes from the tags\n",
    "    # the tags we are interested in are defined in the `tags` list\n",
    "    for tag in tags:\n",
    "        for script in soup.find_all(tag):\n",
    "            src = script.get(\"src\")\n",
    "            if src:\n",
    "                src = WAYBACK_BASE_URL + src\n",
    "                sources.append(src)\n",
    "\n",
    "    sources_path = os.path.join(html_dir, \"sources.json\")\n",
    "    print(f\"Found {len(sources)} sources, saving results to {sources_path}\")\n",
    "    with open(sources_path, \"w\") as f:\n",
    "        json.dump(sources, f, indent=2)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract MIDI URLs ###########\n",
    "    ########################################\n",
    "    midi_urls = []\n",
    "    for src in sources:\n",
    "        if src.endswith(\".mid\"):\n",
    "            midi_urls.append(src)\n",
    "\n",
    "    midi_urls_path = os.path.join(html_dir, \"midi.json\")\n",
    "    print(f\"Found {len(midi_urls)} MIDI files, saving results to {midi_urls_path}\")\n",
    "    with open(midi_urls_path, \"w\") as f:\n",
    "        json.dump(midi_urls, f, indent=2)\n",
    "\n",
    "    midi_dir = os.path.join(html_dir, \"mid\")\n",
    "    os.makedirs(midi_dir, exist_ok=True)\n",
    "    print(f\"Dowloading {len(midi_urls)} MIDI files to {midi_dir}\")\n",
    "    for midi_url in midi_urls:\n",
    "        print(\"Downloading MIDI file:\", midi_url)\n",
    "        midi_file_path = os.path.join(midi_dir, os.path.basename(midi_url))\n",
    "        download_file(midi_url, midi_file_path)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract audio ###############\n",
    "    ########################################\n",
    "    audio_urls = []\n",
    "    audio_extensions = [\n",
    "        \".mp3\",\n",
    "        \".wav\",\n",
    "        \".ogg\",\n",
    "        \".aac\",\n",
    "        \".flac\",\n",
    "        \".alac\",\n",
    "        \".aiff\",\n",
    "        \".dsd\",\n",
    "        \".wma\",\n",
    "        \".opus\",\n",
    "        \".m4a\",\n",
    "    ]\n",
    "    for src in sources:\n",
    "        if src.endswith(tuple(audio_extensions)):\n",
    "            audio_urls.append(src)\n",
    "\n",
    "    audio_urls_path = os.path.join(html_dir, \"audio.json\")\n",
    "    print(f\"Found {len(audio_urls)} audio files, saving results to {audio_urls_path}\")\n",
    "    with open(audio_urls_path, \"w\") as f:\n",
    "        json.dump(audio_urls, f, indent=2)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract image URLs ##########\n",
    "    ########################################\n",
    "    image_urls = []\n",
    "    image_extensions = [\n",
    "        \".jpg\",\n",
    "        \".jpeg\",\n",
    "        \".png\",\n",
    "        \".gif\",\n",
    "        \".svg\",\n",
    "        \".webp\",\n",
    "        \".bmp\",\n",
    "        \".ico\",\n",
    "        \".tiff\",\n",
    "        \".tif\",\n",
    "    ]\n",
    "    for src in sources:\n",
    "        if src.endswith(tuple(image_extensions)):\n",
    "            image_urls.append(src)\n",
    "\n",
    "    image_urls_path = os.path.join(html_dir, \"images.json\")\n",
    "    print(f\"Found {len(image_urls)} images, saving results to {image_urls_path}\")\n",
    "    with open(image_urls_path, \"w\") as f:\n",
    "        json.dump(image_urls, f, indent=2)\n",
    "\n",
    "    image_dir = os.path.join(html_dir, \"images\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    print(f\"Dowloading {len(image_urls)} images to {image_dir}\")\n",
    "    for image_url in image_urls:\n",
    "        print(\"Downloading image:\", image_url)\n",
    "        image_file_path = os.path.join(image_dir, os.path.basename(image_url))\n",
    "        download_file(image_url, image_file_path)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract videos #############\n",
    "    ########################################\n",
    "\n",
    "    video_urls = []\n",
    "    video_extensions = [\".mp4\", \".webm\", \".ogg\", \".avi\", \".flv\", \".mov\", \".wmv\", \".mkv\"]\n",
    "    for src in sources:\n",
    "        if src.endswith(tuple(video_extensions)):\n",
    "            video_urls.append(src)\n",
    "\n",
    "    video_urls_path = os.path.join(html_dir, \"videos.json\")\n",
    "    print(f\"Found {len(video_urls)} videos, saving results to {video_urls_path}\")\n",
    "    with open(video_urls_path, \"w\") as f:\n",
    "        json.dump(video_urls, f, indent=2)\n",
    "\n",
    "    video_dir = os.path.join(html_dir, \"videos\")\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    print(f\"Dowloading {len(video_urls)} videos to {video_dir}\")\n",
    "    for video_url in video_urls:\n",
    "        print(\"Downloading video:\", video_url)\n",
    "        video_file_path = os.path.join(video_dir, os.path.basename(video_url))\n",
    "        download_file(video_url, video_file_path)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract Shockwave files ####\n",
    "    ########################################\n",
    "    swf_urls = []\n",
    "    for src in sources:\n",
    "        if src.endswith(\".swf\"):\n",
    "            swf_urls.append(src)\n",
    "\n",
    "    swf_urls_path = os.path.join(html_dir, \"swf.json\")\n",
    "    print(f\"Found {len(swf_urls)} Shockwave files, saving results to {swf_urls_path}\")\n",
    "    with open(swf_urls_path, \"w\") as f:\n",
    "        json.dump(swf_urls, f, indent=2)\n",
    "\n",
    "    swf_dir = os.path.join(html_dir, \"swf\")\n",
    "    os.makedirs(swf_dir, exist_ok=True)\n",
    "    print(f\"Dowloading {len(swf_urls)} Shockwave files to {swf_dir}\")\n",
    "    for swf_url in swf_urls:\n",
    "        print(\"Downloading Shockwave file:\", swf_url)\n",
    "        swf_file_path = os.path.join(swf_dir, os.path.basename(swf_url))\n",
    "        download_file(swf_url, swf_file_path)\n",
    "\n",
    "    ########################################\n",
    "    ########## Extract VRML files ##########\n",
    "    ########################################\n",
    "    vrml_urls = []\n",
    "    for src in sources:\n",
    "        if src.endswith(\".wrl\"):\n",
    "            vrml_urls.append(src)\n",
    "\n",
    "    vrml_urls_path = os.path.join(html_dir, \"vrml.json\")\n",
    "    print(f\"Found {len(vrml_urls)} VRML files, saving results to {vrml_urls_path}\")\n",
    "    with open(vrml_urls_path, \"w\") as f:\n",
    "        json.dump(vrml_urls, f, indent=2)\n",
    "\n",
    "    vrml_dir = os.path.join(html_dir, \"vrml\")\n",
    "    os.makedirs(vrml_dir, exist_ok=True)\n",
    "    print(f\"Dowloading {len(vrml_urls)} VRML files to {vrml_dir}\")\n",
    "    for vrml_url in vrml_urls:\n",
    "        print(\"Downloading VRML file:\", vrml_url)\n",
    "        vrml_file_path = os.path.join(vrml_dir, os.path.basename(vrml_url))\n",
    "        download_file(vrml_url, vrml_file_path)\n",
    "\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
