{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch snapshots of urls on Wayback Machine\n",
    "- Load url from `.json` file\n",
    "- Fetch snapshots of urls on Wayback Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load urls and id from `.json` file\n",
    "\n",
    "From the previous sketch, we have a `urls.json` file that contains the urls and their unique id. We will load the urls and their id from the file.\n",
    "\n",
    "```python\n",
    "[{'id': \"abcdef\", 'url': 'https://www.example.com/page1'},\n",
    " {'id': \"abceef\", 'url': 'https://www.example.com/page2'},\n",
    " ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Read URLs from a JSON file\n",
    "########################\n",
    "import json\n",
    "\n",
    "URL_PATH = \"urls.json\"\n",
    "\n",
    "# INITIALIZE THE URL LIST\n",
    "url_list = []\n",
    "\n",
    "with open(URL_PATH, \"r\") as f:\n",
    "    url_list = json.load(f)\n",
    "\n",
    "print(f\"Read {len(url_list)} URLs from {URL_PATH}\")\n",
    "print(\"First 5 URLs:\")\n",
    "print(url_list)\n",
    "for url in url_list[:5]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch available snapshots from Wayback Machine\n",
    "Wayback Machine provides an API to fetch the available snapshots of a url, called the [Wayback CDX Server](https://archive.org/developers/wayback-cdx-server.html#basic-usage). By query this API programmatically, we can get the available snapshots of a url in the archive. Currently, these fields are available in the CDX Server:\n",
    "```\n",
    "[\"urlkey\",\"timestamp\",\"original\",\"mimetype\",\"statuscode\",\"digest\",\"length\"]\n",
    "```\n",
    "  \n",
    "We will just focus on the `timestamp` field, which is the timestamp of when the snapshot was taken, and the `statuscode` field, which is the [HTTP status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) of the snapshot. \n",
    "- With `url` and `timestamp`, we can construct the url of the snapshot, and scrape the content of the snapshot later. \n",
    "- With `statuscode`, we can filter out the snapshots that are not actually available on the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests is a library for making HTTP requests\n",
    "# here we are just using it to construct a URL with parameters\n",
    "import requests\n",
    "\n",
    "# pandas is a powerful data manipulation library\n",
    "# here we are using it to read the CSV response from the Wayback Machine\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# define the time range for the snapshots\n",
    "from_time = \"19960101\"\n",
    "to_time = \"20051231\"\n",
    "\n",
    "\n",
    "def make_wm_cdx_url(url, from_time=\"19960101\", to_time=\"20051231\"):\n",
    "    \"\"\"\n",
    "    Construct a URL to query the Wayback Machine CDX API\n",
    "    for a given URL and time range\n",
    "    \"\"\"\n",
    "    base_url = \"https://web.archive.org/cdx/search/cdx\"\n",
    "    params = {\n",
    "        \"url\": url,\n",
    "        \"from\": from_time,\n",
    "        \"to\": to_time,\n",
    "    }\n",
    "\n",
    "    # this will create a URL with the parameters\n",
    "    # eg. https://web.archive.org/cdx/search/cdx?url=example.com&from=19960101&to=20051231\n",
    "    url_with_params = requests.Request(\"GET\", base_url, params=params).prepare().url\n",
    "    return url_with_params\n",
    "\n",
    "\n",
    "total_url_count = len(url_list)\n",
    "sleep_time_on_error = 10\n",
    "sleep_time_on_success = 1\n",
    "\n",
    "\n",
    "for i, entry in enumerate(url_list):\n",
    "    print(f\"Processing URL {i+1}/{total_url_count}\")\n",
    "\n",
    "    url = entry[\"url\"]\n",
    "    id = entry[\"id\"]\n",
    "    print(f\"Processing URL {url} with ID {id}\")\n",
    "\n",
    "    # create the URL with parameters\n",
    "    wayback_cdx_url = make_wm_cdx_url(url)\n",
    "    print(f\"Wayback cdx url {wayback_cdx_url}\")\n",
    "\n",
    "    url_snapshots = []\n",
    "\n",
    "    # we will try to fetch the URL multiple times in case of errors\n",
    "    max_tries = 5\n",
    "\n",
    "    for j in range(max_tries):\n",
    "        # because we are sending requests to the internet, there are many things that can go wrong\n",
    "        # we use a try/except block to catch any errors that occur\n",
    "        # this is important because if one URL fails, we don't want the whole script to stop\n",
    "        print(f\"Try {j+1}/{max_tries}\")\n",
    "        try:\n",
    "            # pandas fetch the CSV from the URL and parse it into a DataFrame\n",
    "            # dataframe is the main data structure in pandas\n",
    "            dataframe = pd.read_csv(\n",
    "                wayback_cdx_url,\n",
    "                names=[\n",
    "                    \"urlkey\",\n",
    "                    \"timestamp\",\n",
    "                    \"original\",\n",
    "                    \"mimetype\",\n",
    "                    \"statuscode\",\n",
    "                    \"digest\",\n",
    "                    \"length\",\n",
    "                ],\n",
    "                sep=\"\\s+\",\n",
    "            )\n",
    "\n",
    "            # we convert the dataframe to a list of dictionaries\n",
    "            snapshot_list = dataframe.to_dict(\"records\")\n",
    "\n",
    "            print(f\"Found {len(snapshot_list)} snapshots for {url}\")\n",
    "\n",
    "            # only keep the timestamp and status code for each snapshot\n",
    "            for snapshot in snapshot_list:\n",
    "                url_snapshots.append(\n",
    "                    {\n",
    "                        \"timestamp\": snapshot[\"timestamp\"],\n",
    "                        \"statuscode\": snapshot[\"statuscode\"],\n",
    "                    }\n",
    "                )\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {wayback_cdx_url}: {e}\")\n",
    "            print(f\"Sleeping for {sleep_time_on_error} seconds\")\n",
    "            sleep(sleep_time_on_error)\n",
    "\n",
    "    print(f\"First 5 snapshots for {url}\")\n",
    "    for snapshot in url_snapshots[:5]:\n",
    "        print(snapshot)\n",
    "    entry[\"snapshots\"] = url_snapshots\n",
    "\n",
    "    # sleep for a while to avoid hitting the Wayback Machine too hard\n",
    "    print(f\"Sleeping for {sleep_time_on_success} seconds\")\n",
    "    sleep(sleep_time_on_success)\n",
    "    clear_output()\n",
    "\n",
    "print(\"First 5 entries:\")\n",
    "for entry in url_list[:5]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process, filter and save available snapshots\n",
    "We want to filter out the snapshots that are not available on the Wayback Machine. We also want to aggregate the `timestamp` and `url` to retreve an accessible Wayback Machine url to the exact snapshot. We will save the available snapshots in a `.json` file. \n",
    "\n",
    "You might notice there is an `if_` in the Wayback Machine url. This is because having `if_` in the url will hide the default toolbar of the Wayback Machine, which is useful when we want to scrape the content of the snapshot.\n",
    "\n",
    "We also filters out the snapshots that are not available on the Wayback Machine. Based on the `statuscode`, we will only keep the snapshots that have `statuscode` of `200`. \n",
    "\n",
    "For this workshop, we are also sampling a smaller set of snapshots to scrape to save time. We will randomly take 2 snapshots from the available snapshots.\n",
    "\n",
    "```python\n",
    "[{'id': \"abcdef\", 'url': 'https://www.example.com/page1', 'snapshots': [{'timestamp': '20210101000000', 'url': 'https://web.archive.org/web/20210101000000if_/https://www.example.com/page1'}]},\n",
    " {'id': \"abceef\", 'url': 'https://www.example.com/page2', 'snapshots': [{'timestamp': '20210101000000', 'url': 'https://web.archive.org/web/20210101000000if_/https://www.example.com/page2'}]},\n",
    " ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "url_with_snapshots_path = \"urls_with_snapshots.json\"\n",
    "print(url_list)\n",
    "\n",
    "\n",
    "new_url_list = []\n",
    "snapshot_sample_size = 2\n",
    "\n",
    "#\n",
    "for entry in url_list:\n",
    "    snapshots = entry[\"snapshots\"]\n",
    "\n",
    "    # only keep the snapshots with status code 200\n",
    "    available_snapshots = []\n",
    "\n",
    "    # only keep the snapshots with status code 200\n",
    "    for snapshot in snapshots:\n",
    "        if snapshot[\"statuscode\"] == 200:\n",
    "            # construct the URL to the snapshot\n",
    "            snapshot_url = (\n",
    "                f\"https://web.archive.org/web/{snapshot['timestamp']}if_/{entry['url']}\"\n",
    "            )\n",
    "            new_snapshot = {\n",
    "                \"timestamp\": snapshot[\"timestamp\"],\n",
    "                \"statuscode\": snapshot[\"statuscode\"],\n",
    "                \"url\": snapshot_url,\n",
    "            }\n",
    "            available_snapshots.append(new_snapshot)\n",
    "\n",
    "    # if there are no snapshots, we don't need to keep this URL\n",
    "    if len(available_snapshots) == 0:\n",
    "        continue\n",
    "\n",
    "    # randomly sample 10 snapshots\n",
    "    # because scraping all snapshots can be slow\n",
    "    available_snapshots = random.sample(\n",
    "        available_snapshots, min(snapshot_sample_size, len(available_snapshots))\n",
    "    )\n",
    "\n",
    "    new_entry = {\n",
    "        \"url\": entry[\"url\"],\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"snapshots\": available_snapshots,\n",
    "    }\n",
    "    new_url_list.append(new_entry)\n",
    "\n",
    "\n",
    "print(f\"Kept {len(new_url_list)} URLs with snapshots\")\n",
    "print(\"First 5 entries:\")\n",
    "for entry in new_url_list[:5]:\n",
    "    print(entry)\n",
    "\n",
    "\n",
    "#######################\n",
    "# WRITE THE NEW LIST TO A FILE\n",
    "#######################\n",
    "print(f\"Writing URLs with snapshots to {url_with_snapshots_path}\")\n",
    "with open(url_with_snapshots_path, \"w\") as f:\n",
    "    json.dump(new_url_list, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
